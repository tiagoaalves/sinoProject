rpart.plot(dt@object)
head(cals)
summary(cals)
# Loading package
library(e1071)
library(caTools)
library(caret)
# Splitting data into train
# and test data
split <- sample.split(cals, SplitRatio = 0.7)
train_cl <- subset(cals, split == "TRUE")
test_cl <- subset(cals, split == "FALSE")
head(train_cl)
# Feature Scaling
train_scale <- scale(train_cl[, 1:3])
test_scale <- scale(test_cl[, 1:3])
head(test_cl)
# Fitting Naive Bayes Model
# to training dataset
set.seed(120)  # Setting Seed
classifier_cl <- naiveBayes(result ~ ., data = train_cl)
classifier_cl
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = test_cl)
y_pred
runApp()
library(arules)
banco <- read.csv("bank-full.csv")
View(banco)
View(banco)
runApp()
bank_full <- read.csv("bank-full.csv", sep=";")
banco <- read.csv("bank-full.csv")
View(banco)
bank_full <- read.csv("bank-full.csv", sep=";")
banco <- read.csv("bank-full.csv")
View(banco)
banco <- read.csv("bank-full.csv", sep=";")
banco <- read.csv("banco-full.csv", sep=";")
banco$saldos <- 0
banco <- read.csv("bank-full.csv", sep=";")
banco$saldos <- 0
saldos <- banco$saldos
for (i in 1:nrow(banco)) {
if(banco$balance[i]<0) {
banco$saldos[i] <- "negativo"
} else if(banco$balance[i]>=0 & banco$balance[i]<=1000) {
banco$saldos[i] <- "ate1000"
} else if(banco$balance[i]>1000 & banco$balance[i] <= 5000) {
banco$ saldos[i] <- "ate5000"
} else if(banco$balance[i]>5000 & banco$balance[i] <= 25000){
banco$saldos[i] <- "ate25000"
} else if(banco$balance[i]>25000 & banco$balance[i] <= 50000){
banco$saldos[i] <- "ate50000"
}else{
banco$saldos[i] <- "mais de 50000"
}
}
banco = subset(banco, select = -c(day, month, duration) )
banco= as(banco,"transactions")
class(banco)
# show most frequent items:
itemFrequencyPlot(banco,support=0.1)
#todas as regras
allRules <- apriori(banco, parameter = list(supp = 0.001, conf = 0.5))
inspect(head(allRules, 100))
#rules with high confidence
rules_Highconf <- sort (allRules, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_Highconf, 50)) # show the support, lift and confidence for all rules
#rules of saying yes
rulesYes <- apriori(data=banco, parameter=list (supp=0.001,conf = 0.08, maxlen= 4), appearance = list (default="lhs",rhs="y=yes"), control = list (verbose=F)) # get rules that lead to buying the product
rules_confYes <- sort (rulesYes, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_confYes))
#rules of saldos ate 1000
rulesBal <- apriori(data=banco, parameter=list (supp=0.001,conf = 0.08, maxlen= 3), appearance = list (default="lhs",rhs="saldos=ate1000"), control = list (verbose=F)) # get rules that lead to buying the product
rules_confBal <- sort (rulesBal, by="confidence", decreasing=TRUE) # 'high-confidence' rules.
inspect(head(rules_confBal))
# eclat algorithm:
par2=list(support=0.006,minlen=2)
e1=eclat(banco,parameter=par2)
inspect(sort(e1,by="support")[1:10])
write(e1,file="erules.csv",sep=",",quote=TRUE,row.names=FALSE)
library(arulesViz)
plot(rulesYes)
# interactive exploration:
ruleExplorer(rulesYes)
ruledfYes = data.frame(
lhs = labels(lhs(rules_confYes)),
rhs = labels(rhs(rules_confYes)),
rules_confYes@quality)
ruledfYes = data.frame(
lhs = labels(lhs(rules_confYes)),
rhs = labels(rhs(rules_confYes)),
rules_confYes@quality)
head(ruledfYes)
ruledfBal = data.frame(
lhs = labels(lhs(rules_confBal)),
rhs = labels(rhs(rules_confBal)),
rules_confBal@quality)
runApp()
View(ruledfBal)
View(ruledfYes)
runApp()
View(ruledfYes)
runApp()
runApp('C:/Users/tiago/Downloads')
runApp('C:/Users/tiago/universidade/4o ano/SINO/entrega')
runApp()
runApp()
shiny::runApp()
library(rminer)
library(rpart)
library(rpart.plot)
library(ROSE)
# classification demo, using wine quality from UCI:
w=read.csv("~/universidade/4 ano/1 semestre/SINO/pl/codigo/bank-full.csv",header=TRUE, stringsAsFactors=TRUE)
library(rminer)
library(rpart)
library(rpart.plot)
library(ROSE)
# classification demo, using wine quality from UCI:
w=read.csv("~/universidade/4 ano/1 semestre/SINO/pl/codigo/bank-full.csv",header=TRUE, stringsAsFactors=TRUE)
library(rpart)
library(rpart.plot)
library(ROSE)
# classification demo, using wine quality from UCI:
w <- read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w= subset(w, select = -c(day, month, duration) )
#load the target as a no and yes countable
#w$y <- ifelse(w$y =='yes',1,0)
#w$y=cut(w$y, c(-1,0.5,1), c("no","yes"))
w1 <- w[25001:35000,]
s1=sample(1:15000,1000)
s2=sample(35001:45000,200)
w2 <- w[s2,]
holdoutM=holdout(w1$y, 2/3, seed=1)
# select random sample of 500 examples (to speedup execution)
print(summary(w1))
#mpause("fit a lr")
lr=fit(y~.,w1[holdoutM$tr,],model="lr")
P=predict(svm,w1[holdoutM$ts,])
P=predict(lr,w1[holdoutM$ts,])
# show leaderboard:
cat("> leaderboard models:",lr@mpar$LB$model,"\n")
cat(">  validation values:",round(lr@mpar$LB$eval,4),"\n")
cat("best model is:",lr@model,"\n")
cat("test set ",metric,"=",round(mmetric(w1$y[holdoutM$ts],P,metric=metric),2),"\n")
# show leaderboard:
cat("> leaderboard models:",lr@mpar$LB$model,"\n")
cat(">  validation values:",round(lr@mpar$LB$eval,4),"\n")
cat("best model is:",lr@model,"\n")
cat("test set ",metric,"=",round(mmetric(w1$y[holdoutM$ts],P,metric=metric),2),"\n")
library(rminer)
library(rpart)
library(rpart.plot)
library(ROSE)
# classification demo, using wine quality from UCI:
w <- read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w= subset(w, select = -c(day, month, duration) )
#load the target as a no and yes countable
#w$y <- ifelse(w$y =='yes',1,0)
#w$y=cut(w$y, c(-1,0.5,1), c("no","yes"))
w1 <- w[25001:35000,]
s1=sample(1:15000,1000)
s2=sample(35001:45000,200)
w2 <- w[s2,]
holdoutM=holdout(w1$y, 2/3, seed=1)
# select random sample of 500 examples (to speedup execution)
print(summary(w1))
#mpause("fit a lr")
lr=fit(y~.,w1[holdoutM$tr,],model="lr")
P=predict(lr,w1[holdoutM$ts,])
# show leaderboard:
cat("> leaderboard models:",lr@mpar$LB$model,"\n")
cat(">  validation values:",round(lr@mpar$LB$eval,4),"\n")
cat("best model is:",lr@model,"\n")
cat("test set ",metric,"=",round(mmetric(w1$y[holdoutM$ts],P,metric=metric),2),"\n")
cat("test set ",metric,"=",round(mmetric(w1$y[holdoutM$ts],P,metric=metric),2),"\n")
plr=predict(lr,w2)
Y=w2[,]$y
#mpause("show some lr metrics:")
print(mmetric(Y,plr,metric=c("ACC","AUC","ACCLASS","AUCCLASS"), D=0.3, TC=2))
print(mmetric(Y,plr,metric="CONF" ))
#mpause("show ROC for svm:")
mgraph(Y,plr,graph="ROC",baseline=TRUE,leg="yes",Grid=10)
#mpause("fit a lr")
lr=fit(y~.,w1[holdoutM$tr,],model="lr")
plr=predict(lr,w2)
Y=w2[,]$y
#mpause("show some lr metrics:")
print(mmetric(Y,plr,metric=c("ACC","AUC","ACCLASS","AUCCLASS"), D=0.3, TC=2))
print(mmetric(Y,plr,metric="CONF" ))
#mpause("show ROC for svm:")
mgraph(Y,plr,graph="ROC",baseline=TRUE,leg="yes",Grid=10)
runApp()
#mpause("fit a lr")
lr=fit(y~.,w1[holdoutM$tr,],model="svm")
plr=predict(lr,w2)
Y=w2[,]$y
#mpause("show some lr metrics:")
print(mmetric(Y,plr,metric=c("ACC","AUC","ACCLASS","AUCCLASS"), D=0.3, TC=2))
print(mmetric(Y,plr,metric="CONF" ))
#mpause("show ROC for svm:")
mgraph(Y,plr,graph="ROC",baseline=TRUE,leg="yes",Grid=10)
#mpause("fit a lr")
lr=fit(y~.,w1[holdoutM$tr,],model="randomForest")
plr=predict(lr,w2)
Y=w2[,]$y
#mpause("show some lr metrics:")
print(mmetric(Y,plr,metric=c("ACC","AUC","ACCLASS","AUCCLASS"), D=0.3, TC=2))
print(mmetric(Y,plr,metric="CONF" ))
#mpause("show ROC for svm:")
mgraph(Y,plr,graph="ROC",baseline=TRUE,leg="yes",Grid=10)
runApp()
#mpause("fit a lr")
lr=fit(y~.,w1[holdoutM$tr,],model="lr")
plr=predict(lr,w2)
Y=w2[,]$y
#mpause("show some lr metrics:")
print(mmetric(Y,plr,metric=c("ACC","AUC","ACCLASS","AUCCLASS"), D=0.3, TC=2))
print(mmetric(Y,plr,metric="CONF" ))
#mpause("show ROC for svm:")
mgraph(Y,plr,graph="ROC",baseline=TRUE,leg="yes",Grid=10)
runApp()
runApp()
lr=fit(y~.,w1[holdoutM$tr,],model=input$var)
plrt<-predict(lrt,w2)
lrt=fit(y~.,w1[holdoutM$tr,],model=input$var)
kmeans(selectedData(), centers = input$clusters)
runApp()
shiny::runApp()
library(rminer)
library(ROSE)
library(rminer)
library(ROSE)
# classification demo, using wine quality from UCI:
w1=read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w1= subset(w1, select = -c(day, month, duration ) )
ws <- w1[30000:40000,]
w2 <- w1[41001:45000,]
ws <- ovun.sample(y~., data=ws, method="both", p=0.5, seed=1)$data
ws <- holdout(ws$y, ratio = 2/3, mode = "stratified")
table(ws$y)
print(summary(ws))
print("### automl3 (might provide better results, requires more computation) ###")
# use of auto-ml, mode 3: "automl3"
inputs=ncol(ws)-1 # number of inputs, needed for random forest
metric="AUC"
sm3=mparheuristic(model="automl3",task="prob",inputs=inputs)
# internal validation method (used within the training data)
imethod=c("kfold",3,123)
ws <- holdout(ws$y, ratio = 2/3, mode = "stratified")
table(ws$y)
print(summary(ws))
print("### automl3 (might provide better results, requires more computation) ###")
# use of auto-ml, mode 3: "automl3"
inputs=ncol(ws)-1 # number of inputs, needed for random forest
metric="AUC"
# classification demo, using wine quality from UCI:
w1=read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w1= subset(w1, select = -c(day, month, duration ) )
ws <- w1[30000:40000,]
w2 <- w1[41001:45000,]
ws <- ovun.sample(y~., data=ws, method="both", p=0.5, seed=1)$data
ws <- holdout(ws$y, ratio = 2/3, mode = "stratified")
table(ws$y)
print(summary(ws))
print("### automl3 (might provide better results, requires more computation) ###")
# use of auto-ml, mode 3: "automl3"
inputs=ncol(ws)-1 # number of inputs, needed for random forest
metric="AUC"
sm3=mparheuristic(model="automl3",task="prob",inputs=inputs)
M3=fit(y~.,data=ws[emethod$tr,],model="auto",search=search3,fdebug=TRUE)
sm3=mparheuristic(model="automl3",task="prob",inputs=inputs)
library(rminer)
library(ROSE)
# classification demo, using wine quality from UCI:
w1=read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w1= subset(w1, select = -c(day, month, duration ) )
ws <- w1[30000:40000,]
w2 <- w1[41001:45000,]
ws <- ovun.sample(y~., data=ws, method="both", p=0.5, seed=1)$data
ws <- holdout(ws$y, ratio = 2/3, mode = "stratified")
table(ws$y)
print(summary(ws))
print("### automl3 (might provide better results, requires more computation) ###")
# use of auto-ml, mode 3: "automl3"
inputs=ncol(ws)-1 # number of inputs, needed for random forest
metric="AUC"
sm3=mparheuristic(model="automl3",task="prob",inputs=inputs)
library(rminer)
library(ROSE)
# classification demo, using wine quality from UCI:
w1=read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w1= subset(w1, select = -c(day, month, duration ) )
wr <- w1[30000:40000,]
w2 <- w1[41001:45000,]
wr <- ovun.sample(y~., data=ws, method="both", p=0.5, seed=1)$data
library(rminer)
library(ROSE)
# classification demo, using wine quality from UCI:
w1=read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w1= subset(w1, select = -c(day, month, duration ) )
wr <- w1[30000:40000,]
w2 <- w1[41001:45000,]
wr <- ovun.sample(y~., data=wr, method="both", p=0.5, seed=1)$data
wr <- holdout(wr$y, ratio = 2/3, mode = "stratified")
table(wr$y)
print(summary(ws))
print("### automl3 (might provide better results, requires more computation) ###")
# use of auto-ml, mode 3: "automl3"
inputs=ncol(wr)-1 # number of inputs, needed for random forest
metric="AUC"
sm3=mparheuristic(model="automl3",task="prob",inputs=inputs)
head(wr)
View(wr)
View(w1)
View(wr)
View(w1)
View(w1)
wr <- w1[30000:40000,]
View(wr)
w2 <- w1[41001:45000,]
wr <- ovun.sample(y~., data=wr, method="both", p=0.5, seed=1)$data
View(wr)
wr <- holdout(wr$y, ratio = 2/3, mode = "stratified")
table(wr$y)
wr <- w1[30000:40000,]
w2 <- w1[41001:45000,]
wr <- ovun.sample(y~., data=wr, method="both", p=0.5, seed=1)$data
wr <- w1[30000:40000,]
w2 <- w1[41001:45000,]
wr <- ovun.sample(y~., data=wr, method="both", p=0.5, seed=1)$data
wr <- holdout(wr$y, ratio = 2/3, mode = "stratified")
View(wr)
table(wr$y)
print(summary(ws))
print("### automl3 (might provide better results, requires more computation) ###")
# use of auto-ml, mode 3: "automl3"
inputs=ncol(wr)-1 # number of inputs, needed for random forest
metric="AUC"
sm3=mparheuristic(model="automl3",task="prob",inputs=inputs)
# internal validation method (used within the training data)
imethod=c("kfold",3,123)
search3=list(search=sm3,smethod="auto",method=imethod,metric=metric,convex=0)
# external validation method:
emethod=holdout(ws$y,2/3,seed=12345) # object with tr and ts indexes
# execution of 1 run of the fast automl1:
print("automl3 fit:")
sm3
library(rminer)
library(ROSE)
# classification demo, using wine quality from UCI:
w1=read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w1= subset(w1, select = -c(day, month, duration ) )
wr <- w1[30000:40000,]
w2 <- w1[41001:45000,]
wr <- ovun.sample(y~., data=wr, method="both", p=0.5, seed=1)$data
wr <- holdout(wr$y, ratio = 2/3, mode = "stratified")
table(wr$y)
print(summary(ws))
print("### automl3 (might provide better results, requires more computation) ###")
# use of auto-ml, mode 3: "automl3"
inputs=ncol(wr)-1 # number of inputs, needed for random forest
metric="AUC"
sm3=mparheuristic(model="automl3",task="prob",inputs=inputs)
print(summary(wr))
library(rminer)
library(ROSE)
# classification demo, using wine quality from UCI:
w1=read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w1= subset(w1, select = -c(day, month, duration ) )
wr <- w1[30000:40000,]
w2 <- w1[41001:45000,]
wr <- ovun.sample(y~., data=wr, method="both", p=0.5, seed=1)$data
table(wr$y)
print(summary(wr))
table(wr$y)
print(summary(wr))
print("### automl3 (might provide better results, requires more computation) ###")
# use of auto-ml, mode 3: "automl3"
inputs=ncol(wr)-1 # number of inputs, needed for random forest
metric="AUC"
sm3=mparheuristic(model="automl3",task="prob",inputs=inputs)
# internal validation method (used within the training data)
imethod=c("kfold",3,123)
search3=list(search=sm3,smethod="auto",method=imethod,metric=metric,convex=0)
# external validation method:
emethod=holdout(ws$y,2/3,seed=12345) # object with tr and ts indexes
# execution of 1 run of the fast automl1:
print("automl3 fit:")
M3=fit(y~.,data=ws[emethod$tr,],model="auto",search=search3,fdebug=TRUE)
library(rminer)
library(ROSE)
# classification demo, using wine quality from UCI:
w1=read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w1= subset(w1, select = -c(day, month, duration ) )
wr <- w1[30000:40000,]
w2 <- w1[41001:45000,]
wr <- ovun.sample(y~., data=wr, method="both", p=0.5, seed=1)$data
table(wr$y)
print(summary(wr))
print("### automl3 (might provide better results, requires more computation) ###")
# use of auto-ml, mode 3: "automl3"
inputs=ncol(wr)-1 # number of inputs, needed for random forest
metric="AUC"
sm3=mparheuristic(model="automl3",task="prob",inputs=inputs)
# internal validation method (used within the training data)
imethod=c("kfold",3,123)
search3=list(search=sm3,smethod="auto",method=imethod,metric=metric,convex=0)
# external validation method:
emethod=holdout(wr$y,2/3,seed=12345) # object with tr and ts indexes
# execution of 1 run of the fast automl1:
print("automl3 fit:")
M3=fit(y~.,data=wr[emethod$tr,],model="auto",search=search3,fdebug=TRUE)
P3=predict(M3,wr[emethod$ts,])
# show leaderboard:
cat("> leaderboard models:",M3@mpar$LB$model,"\n")
cat(">  validation values:",round(M3@mpar$LB$eval,4),"\n")
cat("best model is:",M3@model,"\n")
cat("test set ",metric,"=",round(mmetric(wr$y[emethod$ts],P3,metric=metric),2),"\n")
pse=predict(M3,w2[,])
Y=w2$y
print(mmetric(Y,pse,metric=c("ACC","AUC","ACCLASS","AUCCLASS")))
print(mmetric(Y,pse,metric="CONF" ))
mgraph(Y, pse, graph = "ROC",baseline=TRUE,leg="yes",Grid=10)
w2 <- w1[35000:45000,]
s2=sample(35001:45000,200)
w2 <- w1[s2,]
# show leaderboard:
cat("> leaderboard models:",M3@mpar$LB$model,"\n")
cat(">  validation values:",round(M3@mpar$LB$eval,4),"\n")
cat("best model is:",M3@model,"\n")
cat("test set ",metric,"=",round(mmetric(wr$y[emethod$ts],P3,metric=metric),2),"\n")
pse=predict(M3,w2[,])
Y=w2$y
print(mmetric(Y,pse,metric=c("ACC","AUC","ACCLASS","AUCCLASS")))
print(mmetric(Y,pse,metric="CONF" ))
mgraph(Y, pse, graph = "ROC",baseline=TRUE,leg="yes",Grid=10)
s2=sample(44000:45000,200)
w2 <- w1[s2,]
# show leaderboard:
cat("> leaderboard models:",M3@mpar$LB$model,"\n")
cat(">  validation values:",round(M3@mpar$LB$eval,4),"\n")
cat("best model is:",M3@model,"\n")
cat("test set ",metric,"=",round(mmetric(wr$y[emethod$ts],P3,metric=metric),2),"\n")
pse=predict(M3,w2[,])
Y=w2$y
print(mmetric(Y,pse,metric=c("ACC","AUC","ACCLASS","AUCCLASS")))
print(mmetric(Y,pse,metric="CONF" ))
mgraph(Y, pse, graph = "ROC",baseline=TRUE,leg="yes",Grid=10)
s2=sample(40000:45000,200)
w2 <- w1[s2,]
# show leaderboard:
cat("> leaderboard models:",M3@mpar$LB$model,"\n")
cat(">  validation values:",round(M3@mpar$LB$eval,4),"\n")
cat("best model is:",M3@model,"\n")
cat("test set ",metric,"=",round(mmetric(wr$y[emethod$ts],P3,metric=metric),2),"\n")
pse=predict(M3,w2[,])
Y=w2$y
print(mmetric(Y,pse,metric=c("ACC","AUC","ACCLASS","AUCCLASS")))
print(mmetric(Y,pse,metric="CONF" ))
s2=sample(35000:45000,200)
w2 <- w1[s2,]
# show leaderboard:
cat("> leaderboard models:",M3@mpar$LB$model,"\n")
cat(">  validation values:",round(M3@mpar$LB$eval,4),"\n")
cat("best model is:",M3@model,"\n")
cat("test set ",metric,"=",round(mmetric(wr$y[emethod$ts],P3,metric=metric),2),"\n")
pse=predict(M3,w2[,])
Y=w2$y
print(mmetric(Y,pse,metric=c("ACC","AUC","ACCLASS","AUCCLASS")))
print(mmetric(Y,pse,metric="CONF" ))
mgraph(Y, pse, graph = "ROC",baseline=TRUE,leg="yes",Grid=10)
runApp()
runApp()
runApp()
library(rminer)
library(rpart)
library(rpart.plot)
library(ROSE)
# classification demo, using wine quality from UCI:
w <- read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
library(rminer)
library(rpart)
library(rpart.plot)
library(ROSE)
# classification demo, using wine quality from UCI:
w <- read.csv("bank-full.csv", sep=";",header=TRUE, stringsAsFactors=TRUE)
w= subset(w, select = -c(day, month, duration) )
#load the target as a no and yes countable
#w$y <- ifelse(w$y =='yes',1,0)
#w$y=cut(w$y, c(-1,0.5,1), c("no","yes"))
w1 <- w[25001:35000,]
s1=sample(1:15000,1000)
s2=sample(35001:45000,200)
w2 <- w[s2,]
holdoutM=holdout(w1$y, 2/3, seed=1)
# select random sample of 500 examples (to speedup execution)
print(summary(w1))
#mpause("fit a lr")
lr=fit(y~.,w1[holdoutM$tr,],model="lr")
plr=predict(lr,w2)
Y=w2[,]$y
#mpause("show some lr metrics:")
print(mmetric(Y,plr,metric=c("ACC","AUC","ACCLASS","AUCCLASS"), D=0.3, TC=2))
print(mmetric(Y,plr,metric="CONF" ))
#mpause("show ROC for svm:")
mgraph(Y,plr,graph="ROC",baseline=TRUE,leg="yes",Grid=10)
runApp()
load("C:/Users/tiago/universidade/4o ano/SINO/entrega/.RData")
shiny::runApp()
